{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "d7dd0fe8",
      "metadata": {},
      "source": [
        "# Codeanywhere PyTorch Template\n",
        "\n",
        "Welcome to the PyTorch on Codeanywhere! This template is your springboard for creating amazing image classification projects. We'll use the FashionMNIST dataset – a stylish alternative to the classic MNIST digits – to get you started.\n",
        "\n",
        "In this interactive notebook, you'll learn how to:\n",
        "\n",
        "*   **Load a Dataset:** Grab FashionMNIST and prepare it for our model.\n",
        "*   **Build a Neural Network:** Create a simple but effective network to recognize clothing items.\n",
        "*   **Train Your Model:** Teach your network to identify different types of clothes with a training loop.\n",
        "*   **Test It Out:** See your model in action by testing it on new images. How well does it perform?\n",
        "\n",
        "Ready to become an AI engineer? Let's dive in!\n",
        "\n",
        "The code is based on the [Pytorch build a model tutorial](https://pytorch.org/tutorials/beginner/basics/buildmodel_tutorial.html). You can find more help on the links below:\n",
        "\n",
        "[Learn the Basics](intro.html) ||\n",
        "[Quickstart](quickstart_tutorial.html) ||\n",
        "[Tensors](tensorqs_tutorial.html) ||\n",
        "[Datasets & DataLoaders](data_tutorial.html) ||\n",
        "[Transforms](transforms_tutorial.html) ||\n",
        "**Build Model** ||\n",
        "[Autograd](autogradqs_tutorial.html) ||\n",
        "[Optimization](optimization_tutorial.html) ||\n",
        "[Save & Load Model](saveloadrun_tutorial.html)\n",
        "\n",
        "# Building the Neural Network\n",
        "\n",
        "Neural networks are made up of layers/modules that perform operations on data. The [torch.nn](https://pytorch.org/docs/stable/nn.html) namespace provides all the building blocks you need to create your own neural networks. Every module in PyTorch subclasses the [nn.Module](https://pytorch.org/docs/stable/generated/torch.nn.Module.html). A neural network itself is a module consisting of other modules (layers). This nested structure makes it easy to build and manage complex architectures.\n",
        "\n",
        "In the following sections, we'll build a neural network to classify images in the FashionMNIST dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5c7ea956",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import random"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "53f81fef",
      "metadata": {},
      "source": [
        "## Getting a Device\n",
        "We want to train our model on a hardware accelerator like the GPU, if available. Let's check if [torch.cuda](https://pytorch.org/docs/stable/notes/cuda.html) or [torch.backends.mps](https://pytorch.org/docs/stable/notes/mps.html) are available; otherwise, we'll use the CPU. Codeanywhere may not have GPU support, but we're always looking to improve. Keep an eye out for future updates!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "26e90c82",
      "metadata": {},
      "outputs": [],
      "source": [
        "device = (\n",
        "    \"cuda\"\n",
        "    if torch.cuda.is_available()\n",
        "    else \"mps\"\n",
        "    if torch.backends.mps.is_available()\n",
        "    else \"cpu\"\n",
        ")\n",
        "print(f\"Using {device} device\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bfcdf3ea",
      "metadata": {},
      "source": [
        "## Defining Our Neural Network\n",
        "We define our neural network by subclassing `nn.Module` and initializing the neural network layers in `__init__`. Every `nn.Module` subclass implements the operations on input data in the `forward` method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "517f98d5",
      "metadata": {},
      "outputs": [],
      "source": [
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            nn.Linear(28*28, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 10),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        logits = self.linear_relu_stack(x)\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "98c7b9fe",
      "metadata": {},
      "source": [
        "Let's create an instance of `NeuralNetwork`, move it to our `device`, and print its structure."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "994e05df",
      "metadata": {},
      "outputs": [],
      "source": [
        "model = NeuralNetwork().to(device)\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "50d23871",
      "metadata": {},
      "source": [
        "To use the model, we pass it the input data. This executes the model's `forward` method, along with some [background operations](https://github.com/pytorch/pytorch/blob/270111b7b611d174967ed204776985cefca9c144/torch/nn/modules/module.py#L866). Do not call `model.forward()` directly!\n",
        "\n",
        "Calling the model on the input returns a 2-dimensional tensor with dim=0 corresponding to each output of 10 raw predicted values for each class, and dim=1 corresponding to the individual values of each output. We get the prediction probabilities by passing it through an instance of the `nn.Softmax` module."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "008111eb",
      "metadata": {},
      "outputs": [],
      "source": [
        "X = torch.rand(1, 28, 28, device=device)\n",
        "logits = model(X)\n",
        "pred_probab = nn.Softmax(dim=1)(logits)\n",
        "y_pred = pred_probab.argmax(1)\n",
        "print(f\"Predicted class: {y_pred}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "39a1a661",
      "metadata": {},
      "source": [
        "## Let's dive deeper..."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a013ffe0",
      "metadata": {},
      "source": [
        "## Inside the Model Layers\n",
        "\n",
        "Let's break down the layers in the FashionMNIST model. To illustrate, we'll take a minibatch of 3 images of size 28x28 and see what happens as we pass it through the network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6a93e021",
      "metadata": {},
      "outputs": [],
      "source": [
        "input_image = torch.rand(3,28,28)\n",
        "print(input_image.size())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e725655e",
      "metadata": {},
      "source": [
        "### nn.Flatten\n",
        "We initialize the [nn.Flatten](https://pytorch.org/docs/stable/generated/torch.nn.Flatten.html) layer to convert each 2D 28x28 image into a contiguous array of 784 pixel values. The minibatch dimension (at dim=0) is maintained."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6814e018",
      "metadata": {},
      "outputs": [],
      "source": [
        "flatten = nn.Flatten()\n",
        "flat_image = flatten(input_image)\n",
        "print(flat_image.size())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4a1a6326",
      "metadata": {},
      "source": [
        "### nn.Linear\n",
        "The [linear layer](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html) is a module that applies a linear transformation on the input using its stored weights and biases."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6e493f27",
      "metadata": {},
      "outputs": [],
      "source": [
        "layer1 = nn.Linear(in_features=28*28, out_features=20)\n",
        "hidden1 = layer1(flat_image)\n",
        "print(hidden1.size())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "12be25f4",
      "metadata": {},
      "source": [
        "### nn.ReLU\n",
        "Non-linear activations create complex mappings between the model's inputs and outputs. They're applied after linear transformations to introduce *nonlinearity*, helping neural networks learn a wide variety of phenomena.\n",
        "\n",
        "In this model, we use [nn.ReLU](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html) between our linear layers, but there are other activations to introduce non-linearity in your model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e6e94d29",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"Before ReLU: {hidden1}\\n\\n\")\n",
        "hidden1 = nn.ReLU()(hidden1)\n",
        "print(f\"After ReLU: {hidden1}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "555ab1ea",
      "metadata": {},
      "source": [
        "### nn.Sequential\n",
        "[nn.Sequential](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html) is an ordered container of modules. The data is passed through all the modules in the same order as defined. You can use sequential containers to quickly assemble a network like `seq_modules`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ff0829a1",
      "metadata": {},
      "outputs": [],
      "source": [
        "seq_modules = nn.Sequential(\n",
        "    flatten,\n",
        "    layer1,\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(20, 10)\n",
        ")\n",
        "input_image = torch.rand(3,28,28)\n",
        "logits = seq_modules(input_image)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "787b533a",
      "metadata": {},
      "source": [
        "### nn.Softmax\n",
        "The last linear layer of the neural network returns `logits` - raw values in [-\\infty, \\infty] - which are passed to the [nn.Softmax](https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html) module. The logits are scaled to values [0, 1], representing the model's predicted probabilities for each class. The `dim` parameter indicates the dimension along which the values must sum to 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "48ea2ef1",
      "metadata": {},
      "outputs": [],
      "source": [
        "softmax = nn.Softmax(dim=1)\n",
        "pred_probab = softmax(logits)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f979dfe5",
      "metadata": {},
      "source": [
        "## Model Parameters\n",
        "Many layers inside a neural network are *parameterized*, meaning they have associated weights and biases that are optimized during training. Subclassing `nn.Module` automatically tracks all fields defined inside your model object and makes all parameters accessible using your model's `parameters()` or `named_parameters()` methods.\n",
        "\n",
        "In this example, we iterate over each parameter and print its size and a preview of its values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ea06f082",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"Model structure: {model}\\n\\n\")\n",
        "\n",
        "for name, param in model.named_parameters():\n",
        "    print(f\"Layer: {name} | Size: {param.size()} | Values : {param[:2]} \\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1a67929d",
      "metadata": {},
      "source": [
        "## Training Time!\n",
        "Now let's set up the training loop. First, we'll need a few more things:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "db68c8a1",
      "metadata": {},
      "outputs": [],
      "source": [
        "learning_rate = 1e-3\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "14d04b16",
      "metadata": {},
      "source": [
        "Define the loop functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6dedf9d7",
      "metadata": {},
      "outputs": [],
      "source": [
        "def train(dataloader, model, loss_fn, optimizer):\n",
        "    size = len(dataloader.dataset)\n",
        "    model.train()  # Set the model to training mode\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        X, y = X.to(device), y.to(device)\n",
        "\n",
        "        # Compute prediction error\n",
        "        pred = model(X)\n",
        "        loss = loss_fn(pred, y)\n",
        "\n",
        "        # Backpropagation\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        if batch % 100 == 0:\n",
        "            loss, current = loss.item(), (batch + 1) * len(X)\n",
        "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "def test(dataloader, model, loss_fn):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    model.eval() # Set the model to evaluation mode\n",
        "    test_loss, correct = 0, 0\n",
        "    with torch.no_grad(): # Disable gradient calculation during inference\n",
        "        for X, y in dataloader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            pred = model(X)\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "94ca5472",
      "metadata": {},
      "source": [
        "Now download and prepare the dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cdd1b982",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define the transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(), # Convert images to tensors\n",
        "    transforms.Normalize((0.5,), (0.5,)) # Normalize pixel values\n",
        "])\n",
        "\n",
        "# Download the training and testing datasets\n",
        "training_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=transform\n",
        ")\n",
        "\n",
        "test_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=transform\n",
        ")\n",
        "\n",
        "# Create data loaders\n",
        "batch_size = 64 # Feel free to play with this\n",
        "\n",
        "train_dataloader = DataLoader(training_data, batch_size=batch_size, shuffle=True)\n",
        "test_dataloader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "#Let's see a sample of the training data\n",
        "labels_map = {\n",
        "    0: \"T-Shirt\",\n",
        "    1: \"Trouser\",\n",
        "    2: \"Pullover\",\n",
        "    3: \"Dress\",\n",
        "    4: \"Coat\",\n",
        "    5: \"Sandal\",\n",
        "    6: \"Shirt\",\n",
        "    7: \"Sneaker\",\n",
        "    8: \"Bag\",\n",
        "    9: \"Ankle Boot\",\n",
        "}\n",
        "figure = plt.figure(figsize=(8, 8))\n",
        "cols, rows = 3, 3\n",
        "for i in range(1, cols * rows + 1):\n",
        "    sample_idx = torch.randint(0, len(training_data), size=(1,)).item()\n",
        "    img, label = training_data[sample_idx]\n",
        "    figure.add_subplot(rows, cols, i)\n",
        "    plt.title(labels_map[label])\n",
        "    plt.axis(\"off\")\n",
        "    plt.imshow(img.squeeze(), cmap=\"gray\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "53cef55a",
      "metadata": {},
      "source": [
        "## Run the Training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "493739a9",
      "metadata": {},
      "outputs": [],
      "source": [
        "epochs = 5 #A typical training will involve a lot more epochs, but for demonstration purposes i'll keep it short\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train(train_dataloader, model, loss_fn, optimizer)\n",
        "    test(test_dataloader, model, loss_fn)\n",
        "print(\"Done!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2028ff2a",
      "metadata": {},
      "source": [
        "## Seeing the Results (Inference)\n",
        "\n",
        "Now that we have a trained model, let's see how it performs on some new images from the test dataset. We'll pick a random image and see what the model predicts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ea695965",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get a batch of test images\n",
        "X, y = next(iter(test_dataloader))\n",
        "X, y = X.to(device), y.to(device)\n",
        "\n",
        "# Make predictions\n",
        "model.eval() #Set the model to evaluation mode\n",
        "with torch.no_grad():\n",
        "    pred = model(X)\n",
        "    predicted, actual = labels_map[pred.argmax(1)[0].item()], labels_map[y[0].item()]\n",
        "    print(f'Predicted: \"{predicted}\", Actual: \"{actual}\"')\n",
        "\n",
        "plt.imshow(X[0].cpu().squeeze(), cmap=\"gray\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b3de1a39",
      "metadata": {},
      "source": [
        "## Saving the Model (Persistence)\n",
        "\n",
        "Let's save our trained model so we can use it later without retraining."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "07117246",
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.save(model.state_dict(), \"fashion_model.pth\")\n",
        "print(\"Saved PyTorch Model State to fashion_model.pth\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "10af5716",
      "metadata": {},
      "source": [
        "## Wrapping Up!\n",
        "\n",
        "Congratulations! You've built and trained a fashion classification model using PyTorch and Codeanywhere. This is just the beginning. Experiment with different network architectures, training parameters, and datasets to create even more amazing AI applications!\n",
        "\n",
        "- [torch.nn API](https://pytorch.org/docs/stable/nn.html)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
